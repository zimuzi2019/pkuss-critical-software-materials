{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split train data into train set and valid set\n",
    "train_images, valid_images, train_labels, valid_labels = train_test_split(\n",
    "  np.load('train_images.npy'),\n",
    "  np.load('train_labels.npy'), \n",
    "  test_size = 0.2,\n",
    "  random_state = 42,\n",
    ")\n",
    "\n",
    "test_images = np.load('test_images.npy')\n",
    "test_labels = np.load('test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape, train_labels.shape, valid_images.shape, valid_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumOptimizer:\n",
    "    def __init__(self, parameters, lr=0.01, momentum=0.9):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = [np.zeros_like(p) for p in parameters]\n",
    "    \n",
    "    def update(self, grads):\n",
    "        for i, (param, grad) in enumerate(zip(self.parameters, grads)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grad\n",
    "            param += self.v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, parameters, lr=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = [np.zeros_like(p) for p in parameters]\n",
    "        self.v = [np.zeros_like(p) for p in parameters]\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, grads):\n",
    "        self.t += 1\n",
    "        lr_t = self.lr * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(self.parameters, grads)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "            param -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, optimizer=None):\n",
    "        # Initialize weights and biases\n",
    "        #self.w1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.w1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "        #self.w2 = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.w2 = np.random.randn(hidden_size, hidden_size) * np.sqrt(2. / hidden_size)\n",
    "        self.b2 = np.zeros((1, hidden_size))\n",
    "\n",
    "        #self.w3 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.w3 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "        self.b3 = np.zeros((1, output_size))    \n",
    "\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    # z: (m, hidden_size)\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # z: (m, hidden_size)\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "\n",
    "    # compute the cross-entropy loss\n",
    "    # y_true: true labels     (m, output_size)\n",
    "    # y_pred: predicted labels (m, output_size)\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "        return loss\n",
    "    \n",
    "    # X: input data (m, input_size)\n",
    "    def forward_propagation(self, X):\n",
    "        # z1: X * w1 + b1 (m, hidden_size) \n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "\n",
    "        # z2: a1 * w2 + b2 (m, hidden_size)\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "\n",
    "        # z3: a2 * w2 + b3 (m, output_size)\n",
    "        self.z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "        self.a3 = self.softmax(self.z3)\n",
    "        return self.a3\n",
    "    \n",
    "    # X: input data  (m, input_size)\n",
    "    # Y: true labels (m, output_size)\n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dz3 = self.a3 - y\n",
    "        dw3 = np.dot(self.a2.T, dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "\n",
    "        \n",
    "        #dz2 = np.dot(dz3, self.w3.T) * self.a2 * (1 - self.a2)\n",
    "        dz2 = np.dot(dz3, self.w3.T) * self.relu_derivative(self.z2)\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        #dz1 = np.dot(dz2, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        dz1 = np.dot(dz2, self.w2.T) * self.relu_derivative(self.z1)\n",
    "        dw1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return dw1, db1, dw2, db2, dw3, db3\n",
    "    \n",
    "    \n",
    "    def update_parameters(self, grads, learning_rate=0.01):\n",
    "        if self.optimizer is None:\n",
    "            dw1, db1, dw2, db2, dw3, db3 = grads\n",
    "            self.w1 -= learning_rate * dw1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.w2 -= learning_rate * dw2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            self.w3 -= learning_rate * dw3\n",
    "            self.b3 -= learning_rate * db3\n",
    "        else:\n",
    "            self.optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = 784    # 28x28 pixels\n",
    "hidden_size = 128   # Experiment with different sizes\n",
    "output_size = 10    # Digits 0-9\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = SimpleNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "#optimizer = MomentumOptimizer([nn.w1, nn.b1, nn.w2, nn.b2, nn.w3, nn.b3])\n",
    "#optimizer = AdamOptimizer([nn.w1, nn.b1, nn.w2, nn.b2, nn.w3, nn.b3])\n",
    "#nn.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "def train(model, X_train, y_train, X_val, y_val, X_test, y_test, epochs = 30, learning_rate = 0.01, batch_size = 128, patience = 5, min_delta = 1e-6):\n",
    "    loss_train_history = []\n",
    "    loss_val_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    # early stopping variables\n",
    "    #patience_count = 0\n",
    "    #best_val_loss = np.inf\n",
    "\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_train_batch = X_train[i:i+batch_size]\n",
    "            y_train_batch = y_train[i:i+batch_size]\n",
    "            \n",
    "            # Forward propagation\n",
    "            y_train_pred = model.forward_propagation(X_train_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_train = model.compute_loss(y_train_batch, y_train_pred)\n",
    "            loss_train_history.append(loss_train)\n",
    "            \n",
    "            # Backward propagation\n",
    "            grads = model.backward_propagation(X_train_batch, y_train_batch)\n",
    "            \n",
    "            # Update parameters\n",
    "            model.update_parameters(grads, learning_rate)\n",
    "        \n",
    "            # Validation loss\n",
    "            y_pred_val = model.forward_propagation(X_val)\n",
    "            loss_val = model.compute_loss(y_val, y_pred_val)\n",
    "            loss_val_history.append(loss_val)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            # test accuracy\n",
    "            y_test_pred = model.forward_propagation(X_test)\n",
    "            test_accuracy = np.sum(np.argmax(y_test_pred, axis=1) == np.argmax(y_test, axis=1)) * 1.0 / y_test.shape[0]\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "\n",
    "            print(f\"Epoch {epoch}, Step {step}, Train Loss: {loss_train}, Validation Loss: {loss_val}, Test Set Accuracy: {test_accuracy}\")\n",
    "\n",
    "            # early stopping\n",
    "            #if best_val_loss - loss_val > min_delta:\n",
    "            #    best_val_loss = loss_val\n",
    "            #    patience_count = 0\n",
    "            #else:\n",
    "            #    patience_count += 1\n",
    "\n",
    "            #if patience_count == patience:\n",
    "            #    print(f'Early stopping at epoch {epoch}, step {step}, val loss {loss_val}')\n",
    "            #    return loss_train_history, loss_val_history, test_accuracy_history\n",
    "    return loss_train_history, loss_val_history, test_accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training call\n",
    "loss_train_history, loss_val_history, test_accuracy_history = train(nn, train_images, train_labels, valid_images, valid_labels, test_images, test_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_train_history,label='Train Loss')\n",
    "plt.plot(loss_val_history,label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(test_accuracy_history)\n",
    "plt.title('Test Set Accuracy')\n",
    "plt.text(1950, test_accuracy_history[-1], str(test_accuracy_history[-1]), fontsize=8, ha='right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
