{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# 使用镜像站，这个镜像站要生效需要更新 huggingface_hub 至最新版本\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  \n",
    "\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "data_base_path = \"text-to-code/dataset/concode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = []\n",
    "\n",
    "with open(os.path.join(data_base_path, \"dev.json\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        dev_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "with open(os.path.join(data_base_path, \"train.json\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "with open(os.path.join(data_base_path, \"test.json\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 2000 2000\n",
      "['code', 'nl']\n",
      "{'code': 'boolean function ( ) { return isParsed ; }', 'nl': 'check if details are parsed . concode_field_sep Container parent concode_elem_sep boolean isParsed concode_elem_sep long offset concode_elem_sep long contentStartPosition concode_elem_sep ByteBuffer deadBytes concode_elem_sep boolean isRead concode_elem_sep long memMapSize concode_elem_sep Logger LOG concode_elem_sep byte[] userType concode_elem_sep String type concode_elem_sep ByteBuffer content concode_elem_sep FileChannel fileChannel concode_field_sep Container getParent concode_elem_sep byte[] getUserType concode_elem_sep void readContent concode_elem_sep long getOffset concode_elem_sep long getContentSize concode_elem_sep void getContent concode_elem_sep void setDeadBytes concode_elem_sep void parse concode_elem_sep void getHeader concode_elem_sep long getSize concode_elem_sep void parseDetails concode_elem_sep String getType concode_elem_sep void _parseDetails concode_elem_sep String getPath concode_elem_sep boolean verify concode_elem_sep void setParent concode_elem_sep void getBox concode_elem_sep boolean isSmallBox'}\n",
      "{'code': 'int function ( ) { _total = extractList ( ) . size ( ) ; return _total ; }', 'nl': 'actually walks the bag to make sure the count is correct and resets the running total concode_field_sep Object _current concode_elem_sep int _total concode_elem_sep DefaultMapBag _parent concode_elem_sep Map _map concode_elem_sep int _mods concode_elem_sep Iterator _support concode_field_sep boolean add concode_elem_sep boolean add concode_elem_sep Object next concode_elem_sep boolean containsAll concode_elem_sep boolean containsAll concode_elem_sep void clear concode_elem_sep boolean isEmpty concode_elem_sep boolean hasNext concode_elem_sep void remove concode_elem_sep boolean remove concode_elem_sep boolean remove concode_elem_sep Map getMap concode_elem_sep int modCount concode_elem_sep boolean contains concode_elem_sep Iterator iterator concode_elem_sep boolean removeAll concode_elem_sep int size concode_elem_sep boolean addAll concode_elem_sep int hashCode concode_elem_sep boolean equals concode_elem_sep Object[] toArray concode_elem_sep Object[] toArray concode_elem_sep Set uniqueSet concode_elem_sep void setMap concode_elem_sep String toString concode_elem_sep int getCount concode_elem_sep List extractList concode_elem_sep boolean retainAll concode_elem_sep boolean retainAll'}\n",
      "{'code': '', 'nl': 'generate mappings for each function node and parameters and variables names associated with it . concode_field_sep int parentScope concode_elem_sep ArrayList functionBracePositions concode_elem_sep ObjArray funcObjects concode_elem_sep int functionNum concode_elem_sep ArrayList functionVarMappings concode_elem_sep int lastTokenCount concode_elem_sep ArrayList replacedTokens concode_field_sep boolean isInScopeChain concode_elem_sep void reset concode_elem_sep void leaveNestingLevel concode_elem_sep String getMappedToken concode_elem_sep String getPreviousTokenMapping concode_elem_sep void collectFuncNodes concode_elem_sep int sourceCompress concode_elem_sep void enterNestingLevel'}\n",
      "\n",
      "=====================================\n",
      "\n",
      "1203 1095 997\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(dev_data), len(test_data))\n",
    "print(list(train_data[0].keys()))\n",
    "print(train_data[0])\n",
    "print(dev_data[0])\n",
    "print(test_data[0])\n",
    "\n",
    "print(\"\\n=====================================\\n\")\n",
    "\n",
    "\n",
    "train_data_sorted = sorted(train_data, key=lambda x: len(x[\"code\"])+ len (x[\"nl\"]))\n",
    "dev_data_sorted = sorted(dev_data, key=lambda x: len(x[\"code\"])+ len (x[\"nl\"]))\n",
    "test_data_sorted = sorted(test_data, key=lambda x: len(x[\"code\"])+ len (x[\"nl\"]))\n",
    "\n",
    "print(len(train_data_sorted[int(len(train_data_sorted) * 0.75)][\"code\"]) + len(train_data_sorted[int(len(train_data_sorted) * 0.75)][\"nl\"]), \n",
    "      len(dev_data_sorted[int(len(dev_data_sorted) * 0.75)][\"code\"]) + len(dev_data_sorted[int(len(dev_data_sorted) * 0.75)][\"nl\"]), \n",
    "      len(test_data_sorted[int(len(test_data_sorted) * 0.75)][\"code\"]) + len(test_data_sorted[int(len(test_data_sorted) * 0.75)][\"nl\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cslee/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({'sep_token': '<|sepoftext|>'})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 81.913344 M\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {num_params * 1.0 / 10**6} M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(examples['nl'] + \"<|sepoftext|>\" + examples['code'], truncation=True, padding='max_length', max_length=1024, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_data = [encode(data) for data in train_data]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"train_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoded_train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 1024])\n",
      "tensor(356)\n"
     ]
    }
   ],
   "source": [
    "print(list(encoded_train_data[0].keys()))\n",
    "\n",
    "print(encoded_train_data[0][\"input_ids\"].shape)\n",
    "print(encoded_train_data[0][\"attention_mask\"].shape)\n",
    "print(sum(encoded_train_data[0][\"attention_mask\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dev_data = [encode(data) for data in dev_data]\n",
    "\n",
    "with open(\"dev_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoded_dev_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
