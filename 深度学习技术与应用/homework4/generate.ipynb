{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "test_data = []\n",
    "\n",
    "with open('text-to-code/dataset/concode/test.json') as f:\n",
    "  for line in f:\n",
    "    test_data.append(json.loads(line))\n",
    "\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 'void function ( ScriptOrFnNode arg0 ) { int loc0 = - 1 ; collectFuncNodes ( arg0 , loc0 , null ) ; }', 'nl': 'generate mappings for each function node and parameters and variables names associated with it . concode_field_sep int parentScope concode_elem_sep ArrayList functionBracePositions concode_elem_sep ObjArray funcObjects concode_elem_sep int functionNum concode_elem_sep ArrayList functionVarMappings concode_elem_sep int lastTokenCount concode_elem_sep ArrayList replacedTokens concode_field_sep boolean isInScopeChain concode_elem_sep void reset concode_elem_sep void leaveNestingLevel concode_elem_sep String getMappedToken concode_elem_sep String getPreviousTokenMapping concode_elem_sep void collectFuncNodes concode_elem_sep int sourceCompress concode_elem_sep void enterNestingLevel', 'input': 'generate mappings for each function node and parameters and variables names associated with it . concode_field_sep int parentScope concode_elem_sep ArrayList functionBracePositions concode_elem_sep ObjArray funcObjects concode_elem_sep int functionNum concode_elem_sep ArrayList functionVarMappings concode_elem_sep int lastTokenCount concode_elem_sep ArrayList replacedTokens concode_field_sep boolean isInScopeChain concode_elem_sep void reset concode_elem_sep void leaveNestingLevel concode_elem_sep String getMappedToken concode_elem_sep String getPreviousTokenMapping concode_elem_sep void collectFuncNodes concode_elem_sep int sourceCompress concode_elem_sep void enterNestingLevel<|sepoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_data)):\n",
    "  test_data[i]['input'] = test_data[i]['nl'] + '<|sepoftext|>'\n",
    "\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cslee/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"results/checkpoint-73500\")\n",
    "\n",
    "tokenizer.add_special_tokens({'sep_token': '<|sepoftext|>'})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "generate mappings for each function node and parameters and variables names associated with it . concode_field_sep int parentScope concode_elem_sep ArrayList functionBracePositions concode_elem_sep ObjArray funcObjects concode_elem_sep int functionNum concode_elem_sep ArrayList functionVarMappings concode_elem_sep int lastTokenCount concode_elem_sep ArrayList replacedTokens concode_field_sep boolean isInScopeChain concode_elem_sep void reset concode_elem_sep void leaveNestingLevel concode_elem_sep String getMappedToken concode_elem_sep String getPreviousTokenMapping concode_elem_sep void collectFuncNodes concode_elem_sep int sourceCompress concode_elem_sep void enterNestingLevel<|sepoftext|>\n",
      "int function ( ) { return funcObjects. size ( ) ; }\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(test_data[0]['input'], return_tensors='pt')\n",
    "\n",
    "print(len(encoded_input['input_ids'][0]))\n",
    "print(test_data[0]['input'])\n",
    "\n",
    "gen_tokens = model.generate(encoded_input['input_ids'], \n",
    "                            max_length=model.config.n_positions,\n",
    "                            pad_token_id=tokenizer.eos_token_id, \n",
    "                            num_return_sequences=1)\n",
    "\n",
    "gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=False)[0]\n",
    "\n",
    "sep_index = gen_text.find('<|sepoftext|>')\n",
    "\n",
    "print(gen_text[sep_index+len(tokenizer.sep_token):-len(tokenizer.eos_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_inputs = [tokenizer.encode(d['input'], return_tensors='pt', truncation=True, max_length=model.config.n_positions) for d in test_data]\n",
    "encoded_inputs = [tokenizer.encode(d['input'], return_tensors='pt', truncation=True, max_length=model.config.n_positions - 64) for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [45:52<00:00,  1.38s/it]  \n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "gen_ans = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(encoded_inputs))):\n",
    "  #if len(encoded_inputs[i][0]) > model.config.n_positions // 100 * 100:\n",
    "  #  gen_ans.append(torch.tensor([]))\n",
    "  #else:\n",
    "  gen_ans.append(model.generate(encoded_inputs[i], \n",
    "                                max_length=model.config.n_positions,\n",
    "                                pad_token_id=tokenizer.eos_token_id, \n",
    "                                num_return_sequences=1)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int function ( ) { return funcObjects. size ( ) ; }\n"
     ]
    }
   ],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_ans, skip_special_tokens=False)\n",
    "gen_code = []\n",
    "\n",
    "for i in gen_text:\n",
    "  sep_index = i.find('<|sepoftext|>')\n",
    "  if sep_index == -1:\n",
    "    gen_code.append('')\n",
    "    continue\n",
    "  \n",
    "  eos_index = i.find(tokenizer.eos_token)\n",
    "  if eos_index == -1:\n",
    "    gen_code.append(i[sep_index+len(tokenizer.sep_token):])\n",
    "  else:\n",
    "    gen_code.append(i[sep_index+len(tokenizer.sep_token):eos_index])\n",
    "\n",
    "print(gen_code[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "test_data_processed = copy.deepcopy(test_data)\n",
    "\n",
    "for i in range(len(gen_code)):\n",
    "  if gen_code[i] == '':\n",
    "    test_data_processed[i]['code'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval dataset\n",
    "\n",
    "dev_data = []\n",
    "\n",
    "with open('text-to-code/dataset/concode/dev.json') as f:\n",
    "  for line in f:\n",
    "    dev_data.append(json.loads(line))\n",
    "\n",
    "for i in range(len(dev_data)):\n",
    "  dev_data[i]['input'] = dev_data[i]['nl'] + '<|sepoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 364/2000 [07:01<18:31,  1.47it/s]  /home/cslee/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 1024, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2000/2000 [41:30<00:00,  1.25s/it]  \n"
     ]
    }
   ],
   "source": [
    "# eval dataset\n",
    "\n",
    "dev_data = []\n",
    "\n",
    "with open('text-to-code/dataset/concode/dev.json') as f:\n",
    "  for line in f:\n",
    "    dev_data.append(json.loads(line))\n",
    "\n",
    "for i in range(len(dev_data)):\n",
    "  dev_data[i]['input'] = dev_data[i]['nl'] + '<|sepoftext|>'\n",
    "\n",
    "encoded_dev_inputs = [tokenizer.encode(d['input'], return_tensors='pt', truncation=True, max_length=model.config.n_positions) for d in dev_data]\n",
    "\n",
    "\n",
    "gen_dev_ans = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(encoded_dev_inputs))):\n",
    "  gen_dev_ans.append(model.generate(encoded_dev_inputs[i], \n",
    "                                    max_length=model.config.n_positions,\n",
    "                                    pad_token_id=tokenizer.eos_token_id, \n",
    "                                    num_return_sequences=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dev_text = tokenizer.batch_decode(gen_dev_ans, skip_special_tokens=False)\n",
    "gen_dev_code = []\n",
    "\n",
    "for i in gen_dev_text:\n",
    "  sep_index = i.find('<|sepoftext|>')\n",
    "  if sep_index == -1:\n",
    "    gen_dev_code.append('')\n",
    "    continue\n",
    "\n",
    "  eos_index = i.find(tokenizer.eos_token)\n",
    "  if eos_index == -1:\n",
    "    gen_dev_code.append(i[sep_index+len(tokenizer.sep_token):])\n",
    "  else:\n",
    "    gen_dev_code.append(i[sep_index+len(tokenizer.sep_token):eos_index])\n",
    "\n",
    "dev_data_processed = copy.deepcopy(dev_data)\n",
    "\n",
    "for i in range(len(gen_dev_code)):\n",
    "  if gen_dev_code[i] == '':\n",
    "    dev_data_processed[i]['code'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 19.68, EM: 8.4\n",
      "BLEU: 19.81, EM: 9.55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import nbimporter\n",
    "from bleu import _bleu\n",
    "import os \n",
    "\n",
    "def cal_bleu(gen_code, test_data):\n",
    "  \n",
    "  total = len(gen_code)\n",
    "  EM = 0.0\n",
    "\n",
    "  # 为了和原 repo 中的 _bleu 函数输入保持一致\n",
    "  with open(\"ground_truth.txt\", \"w\") as wf:\n",
    "    for pred_code, labels_code in zip(gen_code, [d['code'] for d in test_data]):  \n",
    "        pred_code = pred_code.strip()\n",
    "        labels_code = labels_code.strip()\n",
    "        wf.write(labels_code+\"\\n\")\n",
    "        \n",
    "        if pred_code == labels_code:\n",
    "           EM += 1 \n",
    "\n",
    "  with open(\"preds.txt\", \"w\") as wf:\n",
    "      for pred_code in gen_code:\n",
    "         pred_code = pred_code.strip()\n",
    "         wf.write(pred_code+\"\\n\")\n",
    "\n",
    "        \n",
    "  bleu_score = round(_bleu(\"ground_truth.txt\", \"preds.txt\"), 2)\n",
    "        \n",
    "  print(f\"BLEU: {bleu_score}, EM: {round(EM/total*100, 2)}\")\n",
    "\n",
    "  try:\n",
    "      os.remove(\"preds.txt\")\n",
    "      os.remove(\"ground_truth.txt\")\n",
    "  except Exception:\n",
    "      pass\n",
    "  \n",
    "cal_bleu(gen_code, test_data)\n",
    "cal_bleu(gen_code, test_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 18.54, EM: 8.45\n",
      "BLEU: 18.6, EM: 9.05\n"
     ]
    }
   ],
   "source": [
    "cal_bleu(gen_dev_code, dev_data)\n",
    "cal_bleu(gen_dev_code, dev_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
