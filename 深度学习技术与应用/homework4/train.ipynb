{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# 使用镜像站，这个镜像站要生效需要更新 huggingface_hub 至最新版本\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  \n",
    "\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "import pickle\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "data_base_path = \"text-to-code/dataset/concode\"\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cslee/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({'sep_token': '<|sepoftext>'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'sep_token': '<|sepoftext>', 'pad_token': '<|endoftext|>'}\n",
      "50256\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "\n",
    "with open(\"train_text.pkl\", \"rb\") as f:\n",
    "    encoded_train_text = pickle.load(f)\n",
    "with open(\"dev_text.pkl\", \"rb\") as f:\n",
    "    encoed_dev_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 2000\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_train_text), len(encoed_dev_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.text[idx][\"input_ids\"], \n",
    "            \"attention_mask\": self.text[idx][\"attention_mask\"],\n",
    "            \"labels\": self.text[idx][\"input_ids\"]\n",
    "        }\n",
    "\n",
    "train_dataset = CodeDataset(encoded_train_text)\n",
    "dev_dataset = CodeDataset(encoed_dev_text)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=4, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import nbimporter\n",
    "\n",
    "#bleu_module_path = os.path.abspath(os.path.join(\"text-to-code\", \"evaluator\"))\n",
    "#if bleu_module_path not in sys.path:\n",
    "#    sys.path.append(bleu_module_path)\n",
    "\n",
    "from bleu import _bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.nn.functional import cross_entropy\n",
    "import random\n",
    "\n",
    "\n",
    "# 实际训练的时候没有用上，因为太容易显存溢出了\n",
    "def bleu_compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids # predictions (total_eval_examples, seq_len, vocab_size), labels (total_eval_examples, seq_len)\n",
    "   \n",
    "    # 似乎会自动计算验证集损失\n",
    "    #loss = cross_entropy(\n",
    "    #    torch.tensor(predictions).view(-1, predictions.shape[-1]), \n",
    "    #    torch.tensor(labels).view(-1), \n",
    "    #    ignore_index=tokenizer.pad_token_id, \n",
    "    #    reduction=\"mean\"\n",
    "    #)\n",
    "\n",
    "    preds = np.argmax(predictions, axis=-1) # pred (total_eval_examples, seq_len)\n",
    "\n",
    "    #selected_indices = random.sample(range(preds.shape[0]), 1000)\n",
    "    #preds = preds[selected_indices]\n",
    "    #labels = labels[selected_indices]\n",
    "    \n",
    "    sep_token_id = tokenizer.sep_token_id\n",
    "\n",
    "    # 提取出代码部分\n",
    "    preds_sep_indices = np.array([np.argwhere(preds[i] == sep_token_id)[0] if sep_token_id in preds[i] else [preds.shape[1]-1] for i in range(preds.shape[0])]) # (total_eval_examples, 1)\n",
    "    labels_sep_indices = np.array([np.argwhere(labels[i] == sep_token_id)[0] for i in range(labels.shape[0])]) # (total_eval_examples, 1)\n",
    "\n",
    "    preds_sep_indices[:, 0] = 40 # for test\n",
    "\n",
    "    preds_code_part = [preds[i, preds_sep_indices[i][0]+1:] for i in range(preds.shape[0])] # (total_eval_examples, partial_seq_len)\n",
    "    labels_code_part = [labels[i, labels_sep_indices[i][0]+1:] for i in range(labels.shape[0])] # (total_eval_examples, partial_seq_len)\n",
    "\n",
    "    #print(len(preds_code_part[0]), len(labels_code_part[0]))\n",
    "\n",
    "    decoded_preds_code_part = tokenizer.batch_decode(preds_code_part, skip_special_tokens=True)  # (total_eval_examples, partial_seq_len)\n",
    "    decoded_labels_code_part = tokenizer.batch_decode(labels_code_part, skip_special_tokens=True) # (total_eval_examples, partial_seq_len)\n",
    "\n",
    "    # print(decoded_preds_code_part[0])\n",
    "    # print(decoded_labels_code_part[0])\n",
    "\n",
    "    total = len(decoded_labels_code_part)\n",
    "    EM = 0.0\n",
    "\n",
    "    # 为了和原 repo 中的 _bleu 函数输入保持一致\n",
    "    with open(\"ground_truth.txt\", \"w\") as wf:\n",
    "        for pred_code, labels_code in zip(decoded_preds_code_part, decoded_labels_code_part):  \n",
    "            pred_code = pred_code.strip()\n",
    "            labels_code = labels_code.strip()\n",
    "            wf.write(labels_code+\"\\n\")\n",
    "\n",
    "            if pred_code == labels_code:\n",
    "                EM += 1 \n",
    "\n",
    "    with open(\"preds.txt\", \"w\") as wf:\n",
    "        for pred_code in decoded_preds_code_part:\n",
    "            pred_code = pred_code.strip()\n",
    "            wf.write(pred_code+\"\\n\")\n",
    "\n",
    "        \n",
    "    bleu_score = round(_bleu(\"ground_truth.txt\", \"preds.txt\"), 2)\n",
    "        \n",
    "    # print(f\"BLEU: {bleu_score}, EM: {round(EM/total*100, 2)}\")\n",
    "\n",
    "    try:\n",
    "        os.remove(\"preds.txt\")\n",
    "        os.remove(\"ground_truth.txt\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #return {'exact_match': round(EM/total*100, 2), 'bleu_score': bleu_score, 'loss': loss.item()}\n",
    "    return {'exact_match': round(EM/total*100, 2), 'bleu_score': bleu_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'bleu_score': 0.04}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试 bleu_compute_metrics\n",
    "class Test_Eval_Pred:\n",
    "    def __init__(self, predictions, label_ids):\n",
    "        self.predictions = predictions\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "\n",
    "temp_label_ids = np.full((4, 1024), 1000)\n",
    "temp_label_ids[:, 40] = sep_token_id\n",
    "\n",
    "temp_preds_ids = np.random.randn(4, 1024, len(tokenizer))\n",
    "\n",
    "test_eval_pred = Test_Eval_Pred(\n",
    "    predictions=temp_preds_ids,\n",
    "    label_ids=temp_label_ids\n",
    ")\n",
    "\n",
    "bleu_compute_metrics(test_eval_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=4,   \n",
    "    per_device_eval_batch_size=4,\n",
    "   \n",
    "    learning_rate=0.00005,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,   \n",
    "    max_grad_norm=1, \n",
    "    lr_scheduler_type=\"linear\",\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    logging_dir='./logs',          \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    #metric_for_best_model=\"bleu_score\",\n",
    "    #greater_is_better=True,\n",
    "    #prediction_loss_only=False,\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    #compute_metrics=bleu_compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "trainer.train(resume_from_checkpoint = \"./results/checkpoint-41500\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
