{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vllm import LLM, SamplingParams\n",
    "# import json\n",
    "\n",
    "# model = \"/home2/xjw/LLaMA-Factory/models/llama3_lora_sft\"\n",
    "# llm = LLM(model=model, tensor_parallel_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/xjw/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-06 18:46:22,339\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-06 18:46:23 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home2/xjw/LLaMA-Factory/models/Qwen1.5-1.8B', speculative_config=None, tokenizer='/home2/xjw/LLaMA-Factory/models/Qwen1.5-1.8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home2/xjw/LLaMA-Factory/models/Qwen1.5-1.8B)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-06 18:46:26 model_runner.py:146] Loading model weights took 3.4653 GB\n",
      "INFO 06-06 18:46:27 gpu_executor.py:83] # GPU blocks: 22509, # CPU blocks: 1365\n",
      "INFO 06-06 18:46:30 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-06 18:46:30 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-06 18:46:36 model_runner.py:924] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = \"/home2/xjw/LLaMA-Factory/models/Qwen1.5-1.8B\"\n",
    "# lora_path = \"/home2/xjw/LLaMA-Factory/saves/Phi3-4B-4k-Chat/lora/train_2024-06-05-21-06-40/checkpoint-90\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, local_files_only=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(mode_path, local_files_only=True)\n",
    "# lora_model  = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "\n",
    "# llm = LLM(model=lora_model, tensor_parallel_size=1)\n",
    "llm = LLM(model=mode_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(instruction, input, resp1, resp2, result=None, explain=None, ref=None):\n",
    "    rsp = f\"### Response 1:\\n{resp1}\\n\\n### Response 2:\\n{resp2}\"\n",
    "\n",
    "    if input:\n",
    "        input_sequence = f\"Below are two responses for a given task. The task is defined by the Instruction with an Input that provides further context. Evaluate the responses and generate a reference answer for the task.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n{rsp}\\n\\n### Evaluation:\\n\"\n",
    "    else:\n",
    "        input_sequence = f\"Below are two responses for a given task. The task is defined by the Instruction. Evaluate the responses and generate a reference answer for the task.\\n\\n### Instruction:\\n{instruction}\\n\\n{rsp}\\n\\n### Evaluation:\\n\"\n",
    "\n",
    "\n",
    "    output_sequence = f\"{result}\\n\\n### Reason: {explain}\\n\\n### Reference: {ref}\\n\"\n",
    "    \n",
    "    return input_sequence, output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandalm_test_set_path = \"/home2/xjw/PandaLM/data/testset-v1.json\"\n",
    "\n",
    "pandalm_test_set = []\n",
    "with open(pandalm_test_set_path, \"r\") as f:\n",
    "  pandalm_test_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandalm_test_set_prompt = []\n",
    "for i in range(len(pandalm_test_set)):\n",
    "  for j in range(3):\n",
    "    pandalm_test_set_prompt.append(\n",
    "      build_prompt(\n",
    "        instruction=pandalm_test_set[i]['instruction'],\n",
    "        input=pandalm_test_set[i]['input'] if 'input' in pandalm_test_set[i] else None,\n",
    "        resp1=pandalm_test_set[i]['response1'],\n",
    "        resp2=pandalm_test_set[i]['response2'],\n",
    "        result=pandalm_test_set[i]['annotator' + str(j+1)] ,\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\n",
    "\n",
    "inputs = [ prompt_template.format(input = i[0]) for i in pandalm_test_set_prompt[ : : 3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below are two responses for a given task. The task is defined by the Instruction with an Input that provides further context. Evaluate the responses and generate a reference answer for the task.\n",
      "\n",
      "### Instruction:\n",
      "Make the article available in a second language by translating it into it.\n",
      "\n",
      "### Input:\n",
      "Dentistry, also known as dental medicine and oral medicine, is the branch of medicine focused on the teeth, gums, and mouth. It consists of the study, diagnosis, prevention, management, and treatment of diseases, disorders, and conditions of the mouth, most commonly focused on dentition (the development and arrangement of teeth) as well as the oral mucosa. Dentistry may also encompass other aspects of the craniofacial complex including the temporomandibular joint. The practitioner is called a dentist.\n",
      "Translate to French:\n",
      "\n",
      "### Response 1:\n",
      "La dentisterie, également connue sous le nom de médecine dentaire et de médecine orale, est la branche de la médecine qui s'occupe des dents, des gencives et de la cavité buccale. Elle comprend l'étude, le diagnostic, la prévention, le traitement et la prise en charge des maladies, des affections et des conditions de la cavité buccale, y compris les dents et les tissus mous. Le praticien est appelé dentiste.\n",
      "\n",
      "### Response 2:\n",
      "Dentistry, also known as dental medicine and oral medicine, is the branch of medicine focused on the teeth, gums, and mouth. It consists of the study, diagnosis, prevention, management, and treatment of diseases, disorders, and conditions of the mouth, most commonly focused on dentition (the development and arrangement of teeth) as well as the oral mucosa. Dentistry may also encompass other aspects of the craniofacial complex including the temporomandibular joint. The practitioner is called a dentist.\n",
      "\n",
      "### Evaluation:\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "print(inputs[6])\n",
    "print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 999/999 [02:31<00:00,  6.59it/s, Generation Speed: 2761.92 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "stop_tokens = [\"<|end_header_id|>\", \"<|eot_id|>\"]\n",
    "# sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=1024)\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=1024, stop=stop_tokens)\n",
    "\n",
    "completions = llm.generate(inputs, sampling_params=sampling_params)\n",
    "\n",
    "generated_responses = [output.outputs[0].text for output in completions]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/home2/xjw/output/pandalm_test_Qwen1.5-1.8B_set_output.json\"\n",
    "with open(file_path, \"w\") as f:\n",
    "  json.dump(generated_responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "  generated_responses = json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "### Reason: Response 1 is better because it is more concise and clear.\n",
      "\n",
      "### Reference: If you have any questions about my rate, please let me know.\n",
      "\n",
      "\n",
      "### Reason: Response 2 is better because it is more concise and clear.\n",
      "\n",
      "### Reference: If you have any questions, please let me know.\n",
      "\n",
      "### Reason: Response 1 is better because it is more concise and clear.\n",
      "\n",
      "### Reference: If you have any questions about my rate, please let me know.\n",
      "\n",
      "\n",
      "### Reason: Response 2 is better because it is more concise and clear.\n",
      "\n",
      "### Reference: If you have any questions about my rate or if you find it necessary to increase or decrease the scope for this project, please let me know.\n",
      "\n",
      "Tie\n",
      "\n",
      "### Reason: Both responses are good and provide relevant search queries.\n",
      "\n",
      "### Reference: 1. \"how to code\" 2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"best coding language\"\n",
      "Tie\n",
      "\n",
      "### Reason: Both responses provide relevant search queries for coding-related topics.\n",
      "\n",
      "### Reference: 1. \"how to code\" 2. \"best coding language\" 3. \"best coding tutorials\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\" 3. \"best coding language\" 4. \"best coding resources\"\n",
      "2. \"how to code\n"
     ]
    }
   ],
   "source": [
    "print(len(generated_responses))\n",
    "# print(inputs[0])\n",
    "print(generated_responses[0])\n",
    "\n",
    "# print(inputs[1])\n",
    "print(generated_responses[1])\n",
    "\n",
    "# print(inputs[2])\n",
    "# print(generated_responses[2])\n",
    "\n",
    "# print(inputs[3])\n",
    "# print(generated_responses[3])\n",
    "\n",
    "# print(inputs[4])\n",
    "print(generated_responses[4])\n",
    "\n",
    "# print(inputs[5])\n",
    "print(generated_responses[5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generated_responses[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "['1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '2', '1', '1', '1', '1', '0', '0', '2', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '1', '2', '1', '2', '1', '0', '0', '2', '1', '2', '0', '1', '0', '0', '1', '1', '1', '1', '2', '2', '2', '0', '0', '1', '1', '1', '0', '2', '1', '1', '1', '1', '2', '1', '1', '2', '1', '2', '1', '2', '1', '1', '1', '0', '0', '0', '2', '0', '0', '1', '2', '2', '2', '2', '2', '2', '1', '1', '1', '2', '0', '1', '0', '0', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '1', '1', '0', '0', '0', '0', '2', '0', '1', '1', '1', '1', '2', '1', '2', '2', '1', '1', '1', '1', '1', '1', '0', '0', '1', '1', '1', '0', '0', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '1', '1', '0', '1', '1', '1', '0', '1', '1', '0', '1', '0', '2', '1', '0', '1', '2', '1', '2', '0', '2', '1', '1', '1', '1', '2', '2', '2', '2', '0', '0', '0', '0', '2', '1', '1', '0', '1', '1', '1', '1', '2', '1', '0', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '1', '1', '2', '2', '2', '1', '2', '2', '1', '1', '1', '1', '0', '2', '0', '2', '0', '1', '1', '0', '2', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '1', '1', '2', '1', '0', '2', '1', '0', '0', '0', '0', '0', '0', '1', '2', '1', '1', '1', '1', '1', '2', '1', '1', '1', '2', '1', '0', '0', '0', '0', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1', '2', '1', '2', '1', '2', '1', '1', '1', '1', '2', '2', '2', '2', '1', '1', '2', '2', '2', '1', '1', '2', '2', '2', '2', '2', '2', '1', '2', '1', '2', '1', '2', '1', '2', '2', '2', '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1', '2', '2', '1', '0', '0', '1', '1', '1', '1', '1', '2', '1', '1', '1', '1', '0', '1', '1', '2', '2', '1', '2', '2', '0', '1', '1', '0', '2', '0', '0', '2', '0', '0', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1', '2', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '1', '2', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '2', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '1', '1', '2', '1', '1', '1', '1', '2', '2', '1', '1', '2', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '1', '0', '1', '2', '2', '1', '2', '0', '2', '0', '2', '0', '2', '1', '0', '0', '1', '1', '1', '1', '1', '2', '2', '2', '2', '0', '2', '1', '0', '2', '0', '2', '0', '2', '1', '1', '2', '2', '2', '1', '2', '1', '1', '2', '1', '2', '1', '1', '2', '1', '1', '1', '1', '1', '2', '1', '2', '1', '1', '1', '1', '2', '1', '1', '1', '2', '1', '1', '1', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '2', '2', '1', '1', '2', '1', '1', '1', '1', '2', '2', '2', '2', '1', '2', '2', '2', '2', '2', '1', '1', '1', '2', '2', '2', '0', '1', '1', '0', '0', '0', '1', '0', '0', '1', '1', '2', '1', '2', '1', '2', '0', '1', '1', '0', '1', '1', '1', '1', '1', '2', '1', '1', '1', '1', '0', '0', '2', '1', '0', '1', '0', '0', '0', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '0', '2', '1', '2', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '1', '2', '2', '2', '1', '1', '0', '1', '1', '2', '0', '2', '2', '2', '2', '2', '1', '0', '0', '0', '0', '0', '0', '0', '2', '0', '0', '0', '0', '0', '0', '2', '1', '1', '1', '1', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '2', '1', '1', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '2', '1', '2', '2', '1', '0', '0', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '1', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '2', '2', '2', '1', '1', '2', '0', '2', '0', '0', '0', '2', '0', '0', '0', '2', '1', '1', '1', '1', '1', '1', '2', '0', '2', '2', '1', '1', '1', '1', '2', '2', '2', '1', '2', '1', '1', '1', '1', '2', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '1', '0', '1', '0', '1', '1', '2', '1', '1', '1', '1', '1', '1', '1', '2', '2', '1', '1', '2', '1', '1', '2', '1', '1', '1', '1', '1', '2', '1', '2', '2', '2', '0', '0', '2', '1', '2', '1', '1', '2', '1', '1', '2', '1', '2', '2', '2', '1', '1', '1', '1', '1', '1', '0', '0', '1', '2', '2', '1', '2', '1', '2', '1', '2', '2', '1', '1', '2', '2', '1', '1', '0', '1', '0', '1', '1', '0', '1', '0', '1', '1', '2', '1', '0', '1', '1', '1', '1', '1', '1', '2', '1', '0', '1', '1', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '1', '2', '1', '2', '1', '1', '1', '0', '1', '0', '2', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "# ans = [ res.split(\"\\n\\n\")[0] for res in generated_responses]\n",
    "\n",
    "ans = [ res.split(\"\\n\")[0] for res in generated_responses]\n",
    "\n",
    "newlist = []\n",
    "\n",
    "\n",
    "# for s in ans:\n",
    "#     if \"Tie\" in s or \"both\" in s:\n",
    "#         newlist.append(\"0\")\n",
    "#     elif \"1\" in s or \"first\" in s:\n",
    "#         newlist.append(\"1\")\n",
    "#     elif \"2\" in s or \"second\" in s:\n",
    "#         newlist.append(\"2\")\n",
    "\n",
    "for s in ans:\n",
    "    if \"Tie\" in s:\n",
    "        newlist.append(\"0\")\n",
    "    elif \"both\" in s and \"But\" not in s and \"but\" not in s:\n",
    "        newlist.append(\"0\")\n",
    "    elif \"Both\" in s and \"But\" not in s and \"but\" not in s:\n",
    "        newlist.append(\"0\")\n",
    "    elif \"1\" in s or \"first\" in s:\n",
    "        newlist.append(\"1\")\n",
    "    elif \"2\" in s or \"second\" in s:\n",
    "        newlist.append(\"2\")\n",
    "\n",
    "    \n",
    "ans = newlist\n",
    "print(len(ans))\n",
    "print(ans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.82\n",
      "F1: 45.46\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "category_1 = 0\n",
    "pred_1 = 0\n",
    "category_1_correct = 0\n",
    "category_2 = 0\n",
    "pred_2 = 0\n",
    "category_2_correct = 0\n",
    "category_3 = 0\n",
    "pred_3 = 0\n",
    "category_3_correct = 0\n",
    "\n",
    "for j in range(len(ans)):\n",
    "  for i in range(3):\n",
    "    label = pandalm_test_set_prompt[j * 3 + i][1].split(\"\\n\\n\")[0]\n",
    "    if label == ans[j]:\n",
    "        correct += 1\n",
    "        if ans[j] == \"0\":\n",
    "          category_1_correct += 1\n",
    "        if ans[j] == \"1\":\n",
    "          category_2_correct += 1\n",
    "        if ans[j] == \"2\":\n",
    "          category_3_correct += 1\n",
    "    \n",
    "    if ans[j] == \"0\":\n",
    "      pred_1 += 1\n",
    "    if ans[j] == \"1\":\n",
    "      pred_2 += 1\n",
    "    if ans[j] == \"2\":\n",
    "      pred_3 += 1\n",
    "      \n",
    "    if label == \"0\":\n",
    "      category_1 += 1\n",
    "    if label == \"1\":\n",
    "      category_2 += 1\n",
    "    if label == \"2\":\n",
    "      category_3 += 1   \n",
    "\n",
    "print(f\"Accuracy: {correct/len(pandalm_test_set_prompt) * 100:.2f}\")\n",
    "\n",
    "def f1_score(p, r):\n",
    "  return 2 * p * r / (p + r)\n",
    "\n",
    "p1 = category_1_correct / pred_1\n",
    "r1 = category_1_correct / category_1\n",
    "p2 = category_2_correct / pred_2\n",
    "r2 = category_2_correct / category_2\n",
    "p3 = category_3_correct / pred_3\n",
    "r3 = category_3_correct / category_3\n",
    "\n",
    "print(f\"F1: {(f1_score(p1, r1) + f1_score(p2, r2) + f1_score(p3, r3) ) * 100 / 3 :.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
