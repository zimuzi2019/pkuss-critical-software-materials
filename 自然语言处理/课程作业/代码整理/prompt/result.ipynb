{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_llmbar_res():\n",
    "    with open('avg_results.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    max_num = max(data.keys())\n",
    "    return data[max_num], max_num\n",
    "\n",
    "def get_other_res():\n",
    "    with open('result.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    other_data =  [x for x in data if 'LLMBar' not in x]\n",
    "    for x in other_data:\n",
    "        testset_name = None\n",
    "        for name in ['PandaLM', 'auto-j', 'MT-Bench']:\n",
    "            if name in x:\n",
    "                testset_name = name\n",
    "                break\n",
    "        x[\"testset\"] = testset_name\n",
    "        x['metric'] = x[testset_name]\n",
    "        del x[testset_name]\n",
    "    return other_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmbar_res = get_llmbar_res()\n",
    "other_res = get_other_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmbar_res, max_num = get_llmbar_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[{\n",
    "        \"PandaLM\": {\n",
    "            \"Accuracy\": 0.6646666666666665,\n",
    "            \"Precision\": 0.6471062227343476,\n",
    "            \"Recall\": 0.665331998665332,\n",
    "            \"F1\": 0.6560925603971778\n",
    "        },\n",
    "        \"prompt_type\": \"prompt1\"\n",
    "    },\n",
    "    {\n",
    "        \"PandaLM\": {\n",
    "            \"Accuracy\": 0.6846846846846847,\n",
    "            \"Precision\": 0.6638175381503862,\n",
    "            \"Recall\": 0.6846846846846847,\n",
    "            \"F1\": 0.6740896590309133\n",
    "        },\n",
    "        \"prompt_type\": \"prompt1\"\n",
    "    }]\n",
    "\n",
    "y={\n",
    "        \"PandaLM\": {\n",
    "            \"Accuracy\": 0.6646666666666665,\n",
    "            \"Precision\": 0.6471062227343476,\n",
    "            \"Recall\": 0.665331998665332,\n",
    "            \"F1\": 0.6560925603971778\n",
    "        },\n",
    "        \"prompt_type\": \"prompt1\"\n",
    "    }\n",
    "for metric in y[\"PandaLM\"].keys():\n",
    "    y[\"PandaLM\"][metric] = (x[0][\"PandaLM\"][metric]+ x[1][\"PandaLM\"][metric])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"PandaLM\": {\"Accuracy\": 0.6746756756756755, \"Precision\": 0.6554618804423669, \"Recall\": 0.6750083416750083, \"F1\": 0.6650911097140455}, \"prompt_type\": \"prompt1\"}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(y).replace('\\'', '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prompt_CoT',\n",
       " 'prompt_vanila_rule',\n",
       " 'prompt1',\n",
       " 'prompt_vanila',\n",
       " 'prompt1_cn',\n",
       " 'prompt_CoT_rule']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([x['prompt_type'] for x in other_res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_html_table(data_list):\n",
    "    # 定义表格的列名和行名\n",
    "    column_names = ['PandaLM', 'auto-j', 'MT-Bench']\n",
    "    row_names = ['prompt1', 'prompt1_cn', 'prompt_vanila', 'prompt_CoT', 'prompt_vanila_rule', 'prompt_CoT_rule']\n",
    "\n",
    "    # 创建一个字典来存储数据\n",
    "    data_dict = {col: {row: {'Accuracy': '-', 'F1': '-'} for row in row_names} for col in column_names}\n",
    "\n",
    "    # 遍历列表中的每一条数据,并将数据填充到字典中\n",
    "    for item in data_list:\n",
    "        testset = item['testset']\n",
    "        prompt_type = item['prompt_type']\n",
    "        accuracy = item['metric']['Accuracy']\n",
    "        f1 = item['metric']['F1']\n",
    "\n",
    "        if testset in column_names and prompt_type in row_names:\n",
    "            data_dict[testset][prompt_type]['Accuracy'] = round(accuracy * 100, 1)\n",
    "            data_dict[testset][prompt_type]['F1'] = round(f1 * 100, 1)\n",
    "\n",
    "    # 构建 HTML 表格\n",
    "    table_html = '<table>\\n'\n",
    "    table_html += '  <tr>\\n'\n",
    "    table_html += '    <th colspan=\"2\"></th>\\n'\n",
    "    for col in column_names:\n",
    "        table_html += f'    <th colspan=\"2\" style=\"text-align: center;\">{col}</th>\\n'\n",
    "    table_html += '  </tr>\\n'\n",
    "    table_html += '  <tr>\\n'\n",
    "    table_html += '    <th colspan=\"2\">Prompt Type</th>\\n'\n",
    "    for _ in column_names:\n",
    "        table_html += '    <th style=\"text-align: center;\">Accuracy</th>\\n'\n",
    "        table_html += '    <th style=\"text-align: center;\">F1</th>\\n'\n",
    "    table_html += '  </tr>\\n'\n",
    "\n",
    "    for row in row_names:\n",
    "        table_html += '  <tr>\\n'\n",
    "        table_html += f'    <th colspan=\"2\">{row}</th>\\n'\n",
    "        for col in column_names:\n",
    "            accuracy = data_dict[col][row]['Accuracy']\n",
    "            f1 = data_dict[col][row]['F1']\n",
    "            table_html += f'    <td style=\"text-align: center;\">{accuracy}</td>\\n'\n",
    "            table_html += f'    <td style=\"text-align: center;\">{f1}</td>\\n'\n",
    "        table_html += '  </tr>\\n'\n",
    "\n",
    "    table_html += '</table>'\n",
    "\n",
    "    return table_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('table.html', 'w') as f:\n",
    "    f.write(convert_to_html_table(other_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html_table(data_list):\n",
    "    # 定义表格的列名和行名\n",
    "    column_names = ['Natural', 'Neighbor', 'GPTInst', 'GPTOut', 'Manual', 'Average', 'Average_sum']\n",
    "    row_names = ['prompt1', 'prompt1_cn', 'prompt_vanila', 'prompt_CoT', 'prompt_vanila_rule', 'prompt_CoT_rule']\n",
    "\n",
    "    # 创建一个字典来存储数据\n",
    "    data_dict = {row: {col: '-' for col in column_names} for row in row_names}\n",
    "\n",
    "    # 遍历列表中的每一条数据,并将数据填充到字典中\n",
    "    for item in data_list:\n",
    "        prompt_type = item['prompt_type']\n",
    "        for col in column_names[:5]:\n",
    "            if col in item['LLMBar']:\n",
    "                data_dict[prompt_type][col] = round(item['LLMBar'][col]['Accuracy'] * 100, 1)\n",
    "\n",
    "        # 计算第6列的Average\n",
    "        cols_2_5 = [data_dict[prompt_type][col] for col in column_names[1:5]]\n",
    "        cols_2_5 = [val for val in cols_2_5 if isinstance(val, (int, float))]\n",
    "        if cols_2_5:\n",
    "            data_dict[prompt_type][column_names[5]] = round(sum(cols_2_5) / len(cols_2_5), 1)\n",
    "\n",
    "        # 计算第7列的Average\n",
    "        cols_1_5 = [data_dict[prompt_type][col] for col in column_names[:5]]\n",
    "        cols_1_5 = [val for val in cols_1_5 if isinstance(val, (int, float))]\n",
    "        if cols_1_5:\n",
    "            data_dict[prompt_type][column_names[6]] = round(sum(cols_1_5) / len(cols_1_5), 1)\n",
    "\n",
    "    # 构建 HTML 表格\n",
    "    table_html = '<table>\\n'\n",
    "    table_html += '  <tr>\\n'\n",
    "    table_html += '    <th rowspan=\"2\">Prompt Type</th>\\n'\n",
    "    table_html += '    <th rowspan=\"2\">Natural</th>\\n'\n",
    "    table_html += '    <th colspan=\"5\">ADVERSARIAL</th>\\n'\n",
    "    table_html += '    <th rowspan=\"2\">Average</th>\\n'\n",
    "    table_html += '  </tr>\\n'\n",
    "    table_html += '  <tr>\\n'\n",
    "    for col in column_names[1:6]:\n",
    "        table_html += f'    <th>{col}</th>\\n'\n",
    "    table_html += '  </tr>\\n'\n",
    "\n",
    "    for row in row_names:\n",
    "        table_html += '  <tr>\\n'\n",
    "        table_html += f'    <td><b>{row}</b></td>\\n'\n",
    "        for col in column_names:\n",
    "            if col != 'Natural':\n",
    "                table_html += f'    <td style=\"text-align: center;\">{data_dict[row][col]}</td>\\n'\n",
    "            else:\n",
    "                table_html += f'    <td>{data_dict[row][col]}</td>\\n'\n",
    "        table_html += '  </tr>\\n'\n",
    "\n",
    "    table_html += '</table>'\n",
    "\n",
    "    return table_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('table.html', 'w') as f:\n",
    "    f.write(generate_html_table(llmbar_res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
