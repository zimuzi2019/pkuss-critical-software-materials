{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/xjw/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-04 03:26:57,471\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-04 03:26:57 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home2/xjw/LLaMA-Factory/models/llama3_lora_sft', speculative_config=None, tokenizer='/home2/xjw/LLaMA-Factory/models/llama3_lora_sft', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home2/xjw/LLaMA-Factory/models/llama3_lora_sft)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-04 03:27:06 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-04 03:27:07 gpu_executor.py:83] # GPU blocks: 27895, # CPU blocks: 2048\n",
      "INFO 06-04 03:27:09 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-04 03:27:09 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-04 03:27:13 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "\n",
    "model = \"/home2/xjw/LLaMA-Factory/models/llama3_lora_sft\"\n",
    "llm = LLM(model=model, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里改了下，原来 \"if result: \" 的判断会出问题\n",
    "def build_prompt(instruction, input, resp1, resp2, result=None, explain=None, ref=None):\n",
    "    rsp = f\"### Response 1:\\n{resp1}\\n\\n### Response 2:\\n{resp2}\"\n",
    "\n",
    "    if input:\n",
    "        input_sequence = f\"Below are two responses for a given task. The task is defined by the Instruction with an Input that provides further context. Evaluate the responses and generate a reference answer for the task.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n{rsp}\\n\\n### Evaluation:\\n\"\n",
    "    else:\n",
    "        input_sequence = f\"Below are two responses for a given task. The task is defined by the Instruction. Evaluate the responses and generate a reference answer for the task.\\n\\n### Instruction:\\n{instruction}\\n\\n{rsp}\\n\\n### Evaluation:\\n\"\n",
    "\n",
    "\n",
    "    output_sequence = f\"{result}\\n\\n### Reason: {explain}\\n\\n### Reference: {ref}\\n\"\n",
    "    \n",
    "    return input_sequence, output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandalm_test_set_path = \"/home2/xjw/PandaLM/data/testset-v1.json\"\n",
    "\n",
    "pandalm_test_set = []\n",
    "with open(pandalm_test_set_path, \"r\") as f:\n",
    "  pandalm_test_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandalm_test_set_prompt = []\n",
    "for i in range(len(pandalm_test_set)):\n",
    "  for j in range(3):\n",
    "    pandalm_test_set_prompt.append(\n",
    "      build_prompt(\n",
    "        instruction=pandalm_test_set[i]['instruction'],\n",
    "        input=pandalm_test_set[i]['input'] if 'input' in pandalm_test_set[i] else None,\n",
    "        resp1=pandalm_test_set[i]['response1'],\n",
    "        resp2=pandalm_test_set[i]['response2'],\n",
    "        result=pandalm_test_set[i]['annotator' + str(j+1)] ,\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\n",
    "\n",
    "inputs = [ prompt_template.format(input = i[0]) for i in pandalm_test_set_prompt[ : : 3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below are two responses for a given task. The task is defined by the Instruction with an Input that provides further context. Evaluate the responses and generate a reference answer for the task.\n",
      "\n",
      "### Instruction:\n",
      "The sentence you are given might be too wordy, complicated, or unclear. Rewrite the sentence and make your writing clearer by keeping it concise. Whenever possible, break complex sentences into multiple sentences and eliminate unnecessary words.\n",
      "\n",
      "### Input:\n",
      "If you have any questions about my rate or if you find it necessary to increase or decrease the scope for this project, please let me know.\n",
      "\n",
      "### Response 1:\n",
      "If you have any questions about my rate, please let me know.\n",
      "\n",
      "### Response 2:\n",
      "If you have any questions, please let me know.\n",
      "\n",
      "### Evaluation:\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])\n",
    "print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 999/999 [00:58<00:00, 17.18it/s, Generation Speed: 1730.42 toks/s]\n"
     ]
    }
   ],
   "source": [
    "stop_tokens = [\"<|eot_id|>\", \"<|end_of_text|>\"]\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=1024, stop=stop_tokens)\n",
    "\n",
    "completions = llm.generate(inputs, sampling_params=sampling_params)\n",
    "\n",
    "generated_responses = [output.outputs[0].text for output in completions]\n",
    "\n",
    "with open(\"pandalm_test_set_output.json\", \"w\") as f:\n",
    "  json.dump(generated_responses, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"pandalm_test_set_output.json\", \"r\") as f:\n",
    "  generated_responses = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "### Reason: Response 1 is better because it is more concise and still conveys the same meaning as the original sentence.\n",
      "\n",
      "### Reference: If you have any questions about my rate, please let me know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_responses[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "['1', '1', '1', '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '1', '1', '2', '2', '1', '2', '2', '2', '2', '2', '2', '2', '1', '2', '2', '1', '2', '2', '2', '2', '2', '1', '2', '2', '2', '1', '0', '1', '2', '1', '2', '1', '1', '0', '1', '1', '2', '1', '1', '1', '1', '2', '1', '2', '1', '2', '1', '0', '0', '0', '0', '2', '2', '2', '2', '2', '2', '1', '1', '2', '1', '1', '1', '1', '2', '2', '2', '2', '2', '1', '1', '2', '1', '2', '2', '1', '2', '1', '1', '2', '1', '1', '1', '2', '1', '2', '2', '2', '2', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '1', '0', '0', '1', '1', '0', '0', '2', '2', '1', '1', '1', '1', '2', '0', '0', '0', '0', '0', '0', '0', '2', '1', '1', '1', '1', '1', '2', '1', '0', '2', '0', '1', '0', '0', '0', '1', '1', '0', '1', '1', '1', '2', '0', '1', '2', '1', '2', '2', '1', '2', '1', '2', '2', '1', '1', '2', '1', '2', '1', '1', '2', '2', '2', '2', '1', '2', '2', '1', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '1', '2', '1', '1', '1', '1', '2', '0', '2', '2', '2', '2', '1', '2', '2', '2', '2', '1', '2', '2', '0', '0', '0', '0', '2', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '1', '1', '1', '1', '2', '2', '1', '2', '2', '1', '1', '1', '1', '2', '1', '2', '2', '1', '0', '2', '2', '1', '1', '2', '1', '2', '2', '1', '2', '1', '0', '1', '0', '1', '2', '2', '1', '1', '1', '1', '2', '2', '1', '2', '1', '2', '2', '0', '0', '0', '0', '2', '2', '2', '1', '1', '1', '1', '2', '2', '2', '2', '1', '2', '1', '1', '2', '1', '1', '1', '2', '0', '2', '0', '0', '0', '1', '1', '1', '2', '1', '2', '2', '1', '2', '1', '2', '1', '2', '2', '2', '1', '1', '2', '0', '2', '2', '1', '2', '2', '2', '2', '2', '2', '1', '2', '1', '2', '1', '2', '2', '2', '2', '2', '2', '2', '2', '0', '1', '2', '2', '2', '2', '2', '2', '1', '1', '1', '2', '1', '1', '2', '2', '2', '2', '0', '2', '2', '2', '2', '1', '2', '2', '2', '1', '2', '2', '1', '2', '1', '2', '2', '2', '2', '2', '1', '1', '2', '2', '2', '2', '1', '1', '2', '2', '2', '1', '1', '1', '1', '1', '1', '1', '2', '2', '1', '2', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '0', '2', '0', '0', '2', '0', '0', '1', '1', '0', '1', '2', '2', '1', '2', '2', '1', '1', '1', '1', '2', '1', '2', '1', '0', '0', '1', '0', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '2', '1', '1', '2', '1', '2', '2', '2', '2', '1', '1', '2', '2', '1', '0', '2', '2', '2', '1', '1', '2', '1', '1', '1', '1', '2', '2', '2', '2', '1', '2', '1', '1', '1', '2', '2', '2', '1', '2', '2', '2', '2', '1', '1', '1', '2', '1', '1', '1', '1', '0', '2', '2', '2', '1', '2', '2', '2', '2', '1', '2', '2', '2', '1', '1', '1', '2', '1', '1', '2', '2', '2', '2', '0', '0', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '0', '1', '1', '2', '2', '2', '2', '2', '2', '2', '1', '2', '1', '1', '1', '1', '1', '0', '2', '2', '2', '2', '2', '2', '1', '1', '2', '1', '1', '2', '1', '2', '1', '1', '1', '1', '2', '1', '1', '0', '2', '0', '1', '0', '2', '2', '2', '1', '1', '1', '2', '2', '2', '2', '2', '2', '1', '1', '2', '1', '1', '1', '2', '2', '1', '2', '2', '1', '1', '2', '2', '2', '2', '2', '1', '0', '0', '2', '2', '2', '1', '1', '1', '2', '1', '2', '2', '2', '2', '1', '2', '2', '2', '2', '2', '2', '1', '0', '1', '2', '1', '0', '1', '1', '2', '0', '2', '0', '2', '2', '2', '2', '2', '1', '2', '1', '1', '2', '2', '1', '1', '2', '1', '1', '2', '1', '2', '1', '0', '2', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '2', '1', '2', '2', '2', '2', '2', '2', '1', '2', '2', '2', '2', '1', '2', '2', '2', '2', '2', '2', '0', '1', '2', '1', '2', '2', '2', '2', '1', '2', '2', '2', '2', '2', '2', '2', '1', '1', '2', '1', '2', '2', '2', '0', '0', '1', '2', '1', '2', '2', '2', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '0', '2', '2', '1', '2', '2', '0', '2', '2', '2', '2', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '2', '2', '1', '1', '1', '1', '1', '1', '1', '2', '1', '2', '2', '2', '2', '2', '1', '1', '2', '2', '1', '2', '1', '2', '2', '2', '2', '2', '2', '2', '1', '2', '1', '1', '1', '1', '2', '2', '1', '1', '1', '2', '2', '1', '2', '1', '1', '1', '1', '2', '2', '2', '1', '2', '1', '1', '1', '1', '1', '1', '0', '1', '2', '2', '1', '1', '1', '2', '2', '1', '2', '1', '1', '2', '1', '2', '2', '2', '2', '2', '1', '1', '1', '2', '2', '2', '1', '2', '2', '2', '1', '2', '1', '1', '1', '1', '1', '2', '1', '2', '2', '2', '1', '1', '1', '1', '2', '1', '1', '2', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '1', '2', '2', '2', '1', '2', '2', '2', '1', '2', '1', '1', '2', '1', '2', '1', '2', '2', '2', '2', '1', '2', '2', '2', '2', '2', '0', '2', '1', '1', '1', '1', '2', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1', '1', '1', '0', '1', '0', '2', '2', '2', '1', '2', '1', '2', '1', '2', '2', '2', '2', '2', '2', '2', '2', '1', '2', '1', '2', '2']\n"
     ]
    }
   ],
   "source": [
    "ans = [ res.split(\"\\n\\n\")[0] for res in generated_responses]\n",
    "\n",
    "ans = [ \"0\" if a == \"Tie\" else a for a in ans]\n",
    "print(len(ans))\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.34\n",
      "F1: 66.84\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "category_1 = 0\n",
    "pred_1 = 0\n",
    "category_1_correct = 0\n",
    "category_2 = 0\n",
    "pred_2 = 0\n",
    "category_2_correct = 0\n",
    "category_3 = 0\n",
    "pred_3 = 0\n",
    "category_3_correct = 0\n",
    "\n",
    "for j in range(len(pandalm_test_set)):\n",
    "  for i in range(3):\n",
    "    label = pandalm_test_set_prompt[j * 3 + i][1].split(\"\\n\\n\")[0]\n",
    "    if label == ans[j]:\n",
    "        correct += 1\n",
    "        if ans[j] == \"0\":\n",
    "          category_1_correct += 1\n",
    "        if ans[j] == \"1\":\n",
    "          category_2_correct += 1\n",
    "        if ans[j] == \"2\":\n",
    "          category_3_correct += 1\n",
    "    \n",
    "    if ans[j] == \"0\":\n",
    "      pred_1 += 1\n",
    "    if ans[j] == \"1\":\n",
    "      pred_2 += 1\n",
    "    if ans[j] == \"2\":\n",
    "      pred_3 += 1\n",
    "      \n",
    "    if label == \"0\":\n",
    "      category_1 += 1\n",
    "    if label == \"1\":\n",
    "      category_2 += 1\n",
    "    if label == \"2\":\n",
    "      category_3 += 1   \n",
    "\n",
    "print(f\"Accuracy: {correct/len(pandalm_test_set_prompt) * 100:.2f}\")\n",
    "\n",
    "def f1_score(p, r):\n",
    "  return 2 * p * r / (p + r)\n",
    "\n",
    "p1 = category_1_correct / pred_1\n",
    "r1 = category_1_correct / category_1\n",
    "p2 = category_2_correct / pred_2\n",
    "r2 = category_2_correct / category_2\n",
    "p3 = category_3_correct / pred_3\n",
    "r3 = category_3_correct / category_3\n",
    "\n",
    "print(f\"F1: {(f1_score(p1, r1) + f1_score(p2, r2) + f1_score(p3, r3) ) * 100 / 3 :.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
