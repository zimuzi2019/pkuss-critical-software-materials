{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "question_info = []\n",
    "\n",
    "with open(\"/home2/xjw/FastChat/fastchat/llm_judge/data/mt_bench/question.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        question_info.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 82,\n",
       " 'category': 'writing',\n",
       " 'turns': [\"Draft a professional email seeking your supervisor's feedback on the 'Quarterly Financial Report' you prepared. Ask specifically about the data analysis, presentation style, and the clarity of conclusions drawn. Keep the email short and to the point.\",\n",
       "  'Take a moment to evaluate and critique your own response.']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_info[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(question_info[1]['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_info_reverse = {}\n",
    "\n",
    "for i in range(len(question_info)):\n",
    "  s = \"\"\n",
    "  question_info_reverse[question_info[i]['question_id']] = question_info[i]['turns']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.',\n",
       " 'Rewrite your previous response. Start every sentence with the letter A.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_info_reverse[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_info = {\"name\": \"pair-v2-multi-turn\", \"type\": \"pairwise\", \"system_prompt\": \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. You should choose the assistant that follows the user's instructions and answers the user's questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. You should focus on who provides a better answer to the second user question. Begin your evaluation by comparing the responses of the two assistants and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \\\"[[A]]\\\" if assistant A is better, \\\"[[B]]\\\" if assistant B is better, and \\\"[[C]]\\\" for a tie.\", \"prompt_template\": \"<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant A:\\n{answer_a_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant A:\\n{answer_a_2}\\n\\n<|The End of Assistant A's Conversation with User|>\\n\\n\\n<|The Start of Assistant B's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant B:\\n{answer_b_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant B:\\n{answer_b_2}\\n\\n<|The End of Assistant B's Conversation with User|>\", \"description\": \"Prompt for multi-turn general questions\", \"category\": \"general\", \"output_format\": \"[[A]]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'type', 'system_prompt', 'prompt_template', 'description', 'category', 'output_format'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. You should choose the assistant that follows the user\\'s instructions and answers the user\\'s questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. You should focus on who provides a better answer to the second user question. Begin your evaluation by comparing the responses of the two assistants and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_info[\"system_prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|The Start of Assistant A's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant A:\\n{answer_a_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant A:\\n{answer_a_2}\\n\\n<|The End of Assistant A's Conversation with User|>\\n\\n\\n<|The Start of Assistant B's Conversation with User|>\\n\\n### User:\\n{question_1}\\n\\n### Assistant B:\\n{answer_b_1}\\n\\n### User:\\n{question_2}\\n\\n### Assistant B:\\n{answer_b_2}\\n\\n<|The End of Assistant B's Conversation with User|>\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_info[\"prompt_template\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[A]]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_info[\"output_format\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. You should choose the assistant that follows the user's instructions and answers the user's questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. You should focus on who provides a better answer to the second user question. Begin your evaluation by comparing the responses of the two assistants and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.\n",
      "\n",
      "<|The Start of Assistant A's Conversation with User|>\n",
      "\n",
      "### User:\n",
      "{question_1}\n",
      "\n",
      "### Assistant A:\n",
      "{answer_a_1}\n",
      "\n",
      "### User:\n",
      "{question_2}\n",
      "\n",
      "### Assistant A:\n",
      "{answer_a_2}\n",
      "\n",
      "<|The End of Assistant A's Conversation with User|>\n",
      "\n",
      "\n",
      "<|The Start of Assistant B's Conversation with User|>\n",
      "\n",
      "### User:\n",
      "{question_1}\n",
      "\n",
      "### Assistant B:\n",
      "{answer_b_1}\n",
      "\n",
      "### User:\n",
      "{question_2}\n",
      "\n",
      "### Assistant B:\n",
      "{answer_b_2}\n",
      "\n",
      "<|The End of Assistant B's Conversation with User|>\n"
     ]
    }
   ],
   "source": [
    "prompt_template = prompt_template_info[\"system_prompt\"] + \"\\n\\n\" + prompt_template_info[\"prompt_template\"]\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/xjw/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3355\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mt_bench = load_dataset(\"/home2/xjw/mt_bench_human_judgements\")\n",
    "\n",
    "print(len(mt_bench['human']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': Value(dtype='int64', id=None),\n",
       " 'model_a': Value(dtype='string', id=None),\n",
       " 'model_b': Value(dtype='string', id=None),\n",
       " 'winner': Value(dtype='string', id=None),\n",
       " 'judge': Value(dtype='string', id=None),\n",
       " 'conversation_a': [{'content': Value(dtype='string', id=None),\n",
       "   'role': Value(dtype='string', id=None)}],\n",
       " 'conversation_b': [{'content': Value(dtype='string', id=None),\n",
       "   'role': Value(dtype='string', id=None)}],\n",
       " 'turn': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_bench['human'][0]['conversation_a']\n",
    "\n",
    "mt_bench['human'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3355\n"
     ]
    }
   ],
   "source": [
    "instructions = []\n",
    "\n",
    "for i in range(len(mt_bench['human'])):\n",
    "    system_part = \"\"\"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. You should choose the assistant that follows the user's instructions and answers the user's questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. You should focus on who provides a better answer to the second user question. Begin your evaluation by comparing the responses of the two assistants and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\"\"\"\n",
    "    \n",
    "    question_content = question_info_reverse[mt_bench['human'][i]['question_id']]  \n",
    "    \n",
    "    part_a = \"<|The Start of Assistant A's Conversation with User|>\\n\\n\"\n",
    "    for j in range(len(question_content)):\n",
    "        part_a = part_a + \"### User:\\n\" + question_content[j] + \"\\n\\n### Assistant A:\\n\" + mt_bench['human'][i]['conversation_a'][j * 2 +1]['content'] + \"\\n\\n\"\n",
    "    part_a = part_a + \"<|The End of Assistant A's Conversation with User|>\\n\\n\\n\"\n",
    "    \n",
    "    part_b = \"<|The Start of Assistant B's Conversation with User|>\\n\\n\"\n",
    "    for j in range(len(question_content)):\n",
    "        part_b = part_b + \"### User:\\n\" + question_content[j] + \"\\n\\n### Assistant B:\\n\" + mt_bench['human'][i]['conversation_b'][j * 2 +1]['content'] + \"\\n\\n\"\n",
    "    part_b = part_b + \"<|The End of Assistant B's Conversation with User|>\"\n",
    "    \n",
    "    instructions.append([system_part, (part_a + part_b)])\n",
    "    \n",
    "print(len(instructions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. You should choose the assistant that follows the user's instructions and answers the user's questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. You should focus on who provides a better answer to the second user question. Begin your evaluation by comparing the responses of the two assistants and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\n"
     ]
    }
   ],
   "source": [
    "print(instructions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 18:56:54,387\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-05 18:56:54 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home2/xjw/LLaMA-Factory/models/llama3_lora_sft', speculative_config=None, tokenizer='/home2/xjw/LLaMA-Factory/models/llama3_lora_sft', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home2/xjw/LLaMA-Factory/models/llama3_lora_sft)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-05 18:57:02 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-05 18:57:03 gpu_executor.py:83] # GPU blocks: 27895, # CPU blocks: 2048\n",
      "INFO 06-05 18:57:06 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-05 18:57:06 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-05 18:57:11 model_runner.py:924] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model = \"/home2/xjw/LLaMA-Factory/models/llama3_lora_sft\"\n",
    "llm = LLM(model=model, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tmeplate =  \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{input}\\n\\nDirectly output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\n",
    "\n",
    "inputs = [ input_tmeplate.format(system=instruction[0], input=instruction[1]) for instruction in instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user questions. You should choose the assistant that follows the user's instructions and answers the user's questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. You should focus on who provides a better answer to the second user question. Begin your evaluation by comparing the responses of the two assistants and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|The Start of Assistant A's Conversation with User|>\n",
      "\n",
      "### User:\n",
      "Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\n",
      "\n",
      "### Assistant A:\n",
      "I recently had the pleasure of visiting Hawaii and it quickly became one of my favorite places. From the stunning beaches to the lush mountains, this place has it all. The people are incredibly friendly and the culture is alive and well. One of the highlights of my trip was visiting the Polynesian Cultural Center. Here, I was able to learn about the culture of the native Hawaiian people and try my hand at traditional crafts and activities. I also had a chance to explore some of the natural wonders of the island, including the breathtaking Hanauma Bay and the majestic Waimea Canyon. Whether you’re looking for a relaxing beach vacation or an adventure filled with culture and nature, Hawaii is the perfect destination.\n",
      "\n",
      "### User:\n",
      "Rewrite your previous response. Start every sentence with the letter A.\n",
      "\n",
      "### Assistant A:\n",
      "Aloha! I recently had the pleasure of visiting Hawaii and it quickly became one of my favorite places. From the stunning beaches to the lush mountains, this place has it all. The people are incredibly friendly and the culture is alive and well. One of the highlights of my trip was visiting the Polynesian Cultural Center. Here, I was able to learn about the culture of the native Hawaiian people and try my hand at traditional crafts and activities. I also had a chance to explore some of the natural wonders of the island, including the breathtaking Hanauma Bay and the majestic Waimea Canyon. Whether you’re looking for a relaxing beach vacation or an adventure filled with culture and nature, Hawaii is the perfect destination.\n",
      "\n",
      "<|The End of Assistant A's Conversation with User|>\n",
      "\n",
      "\n",
      "<|The Start of Assistant B's Conversation with User|>\n",
      "\n",
      "### User:\n",
      "Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\n",
      "\n",
      "### Assistant B:\n",
      "Aloha! I recently had the pleasure of embarking on a trip to the beautiful island of Hawaii, and let me tell you, the cultural experiences and must-see attractions did not disappoint.\n",
      "\n",
      "First on my list was a visit to the Polynesian Cultural Center. This interactive experience immerses you in the cultures of the Pacific Islands, from the intricate dances of the Maori people of New Zealand to the fire knife dancing of Samoa. The center also features a canoe pageant, where different island nations showcase their unique styles of canoeing. It was truly a feast for the senses and a fascinating insight into the diverse cultures of the Pacific.\n",
      "\n",
      "Next up was a trip to the North Shore, which boasts some of the best surf spots in the world. I watched in awe as surfers of all levels tackled the massive waves, and even had the chance to take a lesson myself. It was an exhilarating experience, and I left with a newfound respect for the power of the ocean.\n",
      "\n",
      "Of course, no trip to Hawaii would be complete without a visit to Pearl Harbor. The somber memorial serves as a reminder of the sacrifices made during World War II, and it was a deeply moving experience to pay my respects to the brave men and women who lost their lives on that fateful day.\n",
      "\n",
      "Last but not least, I made sure to indulge in some of the local cuisine. From poke bowls to shave ice, the flavors of Hawaii are truly unique and delicious. A personal favorite was the plate lunch, which typically consists of a protein, rice, and macaroni salad. It may not be the healthiest option, but it sure is tasty!\n",
      "\n",
      "Overall, my trip to Hawaii was an unforgettable experience. The cultural immersion, natural beauty, and delicious food all contributed to an incredible adventure. If you're looking for a destination that has it all, Hawaii should definitely be at the top of your list. Mahalo for reading!\n",
      "\n",
      "### User:\n",
      "Rewrite your previous response. Start every sentence with the letter A.\n",
      "\n",
      "### Assistant B:\n",
      "Absolutely! A recent trip to the beautiful island of Hawaii was an adventure that I will never forget.\n",
      "\n",
      "As a helpful assistant, I highly recommend a visit to the Polynesian Cultural Center as it immerses you in the cultures of the Pacific Islands.\n",
      "\n",
      "Also, a trip to the North Shore is a must-see attraction, where you can witness surfers of all levels tackling the massive waves.\n",
      "\n",
      "Additionally, paying respects to the brave men and women who lost their lives at Pearl Harbor was a deeply moving experience.\n",
      "\n",
      "And let's not forget about the delicious local cuisine, from poke bowls to shave ice, the flavors of Hawaii are truly unique and tasty.\n",
      "\n",
      "All in all, Hawaii has it all - cultural immersion, natural beauty, and delicious food. As an assistant, I encourage you to put Hawaii at the top of your travel list. Aloha!\n",
      "\n",
      "<|The End of Assistant B's Conversation with User|>\n",
      "\n",
      "Directly output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  68%|██████▊   | 2293/3355 [14:44<21:20,  1.21s/it, Generation Speed: 260.53 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-05 17:57:39 scheduler.py:1077] Sequence group 2480 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3355/3355 [28:44<00:00,  1.95it/s, Generation Speed: 488.76 toks/s]  \n"
     ]
    }
   ],
   "source": [
    "stop_tokens = [\"<|eot_id|>\", \"<|end_of_text|>\"]\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=2048, stop=stop_tokens)\n",
    "\n",
    "completions = llm.generate(inputs, sampling_params=sampling_params)\n",
    "\n",
    "generated_responses = [output.outputs[0].text for output in completions]\n",
    "\n",
    "with open(\"mt_bench_output.json\", \"w\") as f:\n",
    "  json.dump(generated_responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/home2/xjw/mt_bench_output.json\", \"r\") as f:\n",
    "  generated_responses = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "\n",
    "for i in range(len(generated_responses)):\n",
    "    if generated_responses[i].rfind(\"[[A]]\") != -1:\n",
    "        ans.append(\"model_a\")\n",
    "        continue\n",
    "    elif generated_responses[i].rfind(\"[[B]]\") != -1:\n",
    "        ans.append(\"model_b\")\n",
    "        continue\n",
    "    elif generated_responses[i].rfind(\"[[C]]\") != -1:\n",
    "        ans.append(\"tie\")\n",
    "        continue\n",
    "    else:\n",
    "        ans.append(\"No answer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "sum_a = 0\n",
    "sum_c = 0\n",
    "sum_b = 0\n",
    "correct_a = 0\n",
    "correct_b = 0\n",
    "correct_c = 0\n",
    "pred_a = 0\n",
    "pred_b = 0\n",
    "pred_c = 0\n",
    "\n",
    "\n",
    "for i in range(len(ans)):\n",
    "    pred = ans[i]\n",
    "    ground_truth = mt_bench['human'][i]['winner']\n",
    "    if pred == ground_truth:\n",
    "        correct += 1\n",
    "        if pred == \"model_a\":\n",
    "            correct_a += 1\n",
    "        elif pred == \"model_b\":\n",
    "            correct_b += 1\n",
    "        else:\n",
    "            correct_c += 1\n",
    "    \n",
    "    if pred == \"model_a\":\n",
    "        pred_a += 1\n",
    "    elif pred == \"model_b\":\n",
    "        pred_b += 1\n",
    "    elif pred == \"tie\":\n",
    "        pred_c += 1\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"No answer!\")\n",
    "    \n",
    "    if ground_truth == \"model_a\":\n",
    "        sum_a += 1\n",
    "    elif ground_truth == \"model_b\":  \n",
    "        sum_b += 1\n",
    "    else:\n",
    "        sum_c += 1\n",
    "\n",
    "def calc_p(correct, pred):\n",
    "    return correct/pred\n",
    "def calc_r(correct, sum):\n",
    "    return correct/sum\n",
    "def calc_f1(p, r):\n",
    "    return 2*p*r/(p+r)\n",
    "\n",
    "p_a = calc_p(correct_a, pred_a)\n",
    "r_a = calc_r(correct_a, sum_a)\n",
    "f1_a = calc_f1(p_a, r_a)\n",
    "\n",
    "p_b = calc_p(correct_b, pred_b)\n",
    "r_b = calc_r(correct_b, sum_b)\n",
    "f1_b = calc_f1(p_b, r_b)\n",
    "\n",
    "p_c = calc_p(correct_c, pred_c)\n",
    "r_c = calc_r(correct_c, sum_c)\n",
    "\n",
    "if p_c + r_c == 0:\n",
    "    f1_c = 0\n",
    "else:\n",
    "    f1_c = calc_f1(p_c, r_c)\n",
    "\n",
    "    \n",
    "print(f\"Accuracy: {correct*100/len(ans):.2f}\")\n",
    "print(f\"Precision: {(p_a + p_b + p_c)*100/3:.2f}\")\n",
    "print(f\"Recall: {(r_a + r_b + r_c)*100/3:.2f}\")\n",
    "print(f\"F1: {(f1_a + f1_b + f1_c)*100/3:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
